#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Fri Feb  3 09:37:21 2023

@author: ming
"""

import pandas as pd
from fancyimpute import KNN
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report,confusion_matrix
from sklearn import preprocessing
from sklearn.feature_selection import SelectFromModel
from sklearn.metrics import roc_auc_score
from sklearn.model_selection import GridSearchCV

import matplotlib.pyplot as plt
from sklearn.metrics import fowlkes_mallows_score
from sklearn.decomposition import PCA
from sklearn.ensemble import RandomForestRegressor

# import data
X=pd.read_csv()
Y=pd.read_csv()

from sklearn.preprocessing import LabelEncoder
 
# Creating a instance of label Encoder.
le = LabelEncoder()
 
# Using .fit_transform function to fit label
# encoder and return encoded label
gender1 = le.fit_transform(X['gender'])
X.drop("gender", axis=1, inplace=True)
 
# Appending the array to our dataFrame
# with column name 'gender'
X["gender"] = gender1

X.dropna(axis=0, how='any', inplace=True)

Y=X['outcome']

X.drop("outcome", axis=1, inplace=True)


from sklearn.preprocessing import LabelEncoder
 
# Creating a instance of label Encoder.
le = LabelEncoder()
 
# Using .fit_transform function to fit label
# encoder and return encoded label
gender1 = le.fit_transform(X['gender'])
X.drop("gender", axis=1, inplace=True)
 
# Appending the array to our dataFrame
# with column name 'gender'
X["gender"] = gender1


X.drop("outcome", axis=1, inplace=True)

#impute
Xc=X.columns

#standardize
standardized_X = preprocessing.scale(X)
'''
X = pd.DataFrame(KNN(k=6).fit_transform(standardized_X)) 
X.columns=Xc
'''


trainX, testX, trainY, testY = train_test_split(X, Y, random_state=3, test_size=0.2)

#oversampling

#resample
from imblearn.over_sampling import SMOTE
sm = SMOTE(random_state = 42, n_jobs = -1)
X_resampled, y_resampled = sm.fit_resample(trainX, trainY)




models = {
    "knn": KNeighborsClassifier(n_neighbors=1),
    "naive_bayes": GaussianNB(),
    "logit": LogisticRegression(solver="lbfgs", multi_class="auto"),
    "svm": SVC(kernel="rbf", gamma=1),
    "decision_tree": DecisionTreeClassifier(criterion='gini',splitter='random',max_depth=15,min_samples_leaf=10,min_samples_split=10), 
    "random_forest": RandomForestClassifier(n_estimators=20,max_depth=10,max_samples=70,max_features=5),
    "mlp": MLPClassifier()
}

#feature selection

sel = SelectFromModel(RandomForestClassifier(n_estimators = 100)) 
sel.fit(trainX, trainY)



from sklearn.feature_selection import RFE
rfe = RFE(RandomForestRegressor(n_estimators=100,
                                 random_state=38),
         n_features_to_select=15)
rfe.fit(trainX, trainY)
mask = rfe.get_support()

selected_feat1= trainX.columns[(rfe.get_support())]

selected_feat= trainX.columns[(sel.get_support())]
X_select=trainX[selected_feat]

#model implement

print("use '{}' bulid model...".format(""))
model = models[""]

model.fit(X_resampled, y_resampled)

#gridsearch
tree_param_grid = {'n_estimators': range(50,100,10),
                  'min_samples_split':range(30,100,20),
                  'max_features':range(5,25,5),
                  'max_depth':range(3,22,5)
                  }
 
tree_best = GridSearchCV(model,param_grid = tree_param_grid,cv = 3,scoring="roc_auc",n_jobs= -1, verbose = 100) 
#Gridsearch turning parameters  
tree_best.fit(trainX, trainY)

print(tree_best.best_estimator_)
print(tree_best.best_params_,"  ","score：",tree_best.best_score_)


#feature importance
Xc=X.columns
features_importance = pd.concat([pd.DataFrame(Xc).T,pd.DataFrame(model.feature_importances_).T],axis=0).T
features_importance.columns=["feature","importance"]


## prediction
print("elvaluation...")
predictions = model.predict(testX)
print(classification_report(testY, predictions))
auc_score = roc_auc_score(testY, predictions)
print('CONFUSION MATRIX')
print(confusion_matrix(testY, predictions))
print("AUC")
print(auc_score)

#Xgboost model 

#import pakage

from sklearn.metrics import precision_score

import xgboost as xgb
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report
from sklearn.model_selection import GridSearchCV 
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn import preprocessing
# import data
X=pd.read_csv(r"/Users/ming/Downloads/X4.csv")
Y=pd.read_csv(r"/Users/ming/Downloads/outcome.csv")

X1 = pd.get_dummies(X['gender'])
 
# using pd.concat to concatenate the dataframes
# df and df1 and storing the concatenated
# dataFrame in df.
X = pd.concat([X, X1], axis=1).reindex(X.index)
 
# removing the column 'Purchased' from df
# as it is of no use now.
X.drop('gender', axis=1, inplace=True)

X.drop('outcome', axis=1, inplace=True)




# split data
trainX, testX, trainY, testY = train_test_split(X, Y, random_state=3, test_size=0.2)

from imblearn.over_sampling import RandomOverSampler
ros = RandomOverSampler(random_state=0)
X_resampled, y_resampled = ros.fit_resample(trainX, trainY)

model=xgb.XGBClassifier()

model.fit(trainX,trainY)

y_pred = model.predict(testX)
 
 
#evaluation

print('the evaluation report：',
      classification_report(testY,y_pred))

from xgboost import plot_importance
# plt.figure(figsize=(15,15))
plt.rcParams["figure.figsize"] = (14, 8)
plot_importance(model)



# ROC、AUC
from sklearn.metrics import precision_recall_curve
from sklearn import metrics

y_pred_prob=model.predict_proba(testX)[:,1]

fpr, tpr, thresholds = metrics.roc_curve(testY,y_pred_prob, pos_label=1)

# print(roc_auc)
plt.figure(figsize=(8,6))
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.plot(fpr, tpr, 'r',label='AUC = %0.2f'% roc_auc)
plt.legend(loc='lower right')
# plt.plot([0, 1], [0, 1], 'r--')
plt.xlim([0, 1.1])
plt.ylim([0, 1.1])
plt.xlabel('False Positive Rate') 
plt.ylabel('True Positive Rate')  
plt.title('Receiver operating characteristic example')
plt.show()

y_pred_prob1=model.predict_proba(X)[:,1]
y_pred1=model.predict(X)

#10-fold cross validation
from sklearn.model_selection import cross_val_score
scores = cross_val_score(model, X_resampled, y_resampled, cv=10)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))

